# -*- coding: utf-8 -*-
"""2장 도전과제.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Phyuh1W3XQDCnsQ02HmIu9jTlxkvLEG

도전과제1. 요약 모델을 사용해 단락의 요약 생성하기
"""

from transformers import pipeline
summarizer = pipeline("summarization")

movie_introduction = """
    Man-soo (Lee Byung-hun), a paper expert with 25 years of experience,
    was so content with his life that he felt he had accomplished everything.
    He was living a happy life with his wife, Mi-ri (Son Ye-jin),
    two children, and their dog when he was suddenly laid off by his company.
    "I'm sorry. There's nothing I can do." Suffering from the shock of his death,
    Man-soo vows to find a new job within three months for the sake of his family.
    His resolve proves fruitless, as he works at a supermarket for over a year,
    constantly interviewing, and ultimately finds himself in danger of losing
    his hard-earned home. He desperately seeks out Moon Paper and submits
    his resume, only to be humiliated by team leader Seon-chul (Park Hee-soon).
    Convinced that he is the perfect fit for the position at Moon Paper,
    Man-soo resolves: "If there's no position for me, I'll create one, even if
    it means getting hired."
"""

summarizer(movie_introduction)

generator = pipeline("text-generation", model="gpt2")
def summarize_with_zero_shot(prompt):
    zero_shot = generator(
        f"{prompt}.\\n Summary:\\n",
        return_full_text=False,
        max_length=350,
    )
    return zero_shot
summarize_with_zero_shot(movie_introduction)

generator = pipeline("text-generation", model="gpt2")
def summarize_with_few_shot(prompt):
    few_shot = generator(
        f"""
        America has changed dramatically during recent years. Not only has the number of
    graduates in traditional engineering disciplines such as mechanical, civil,
    electrical, chemical, and aeronautical engineering declined, but in most of
    the premier American universities engineering curricula now concentrate on
    and encourage largely the study of engineering science. As a result, there
    are declining offerings in engineering subjects dealing with infrastructure,
    the environment, and related issues, and greater concentration on high
    technology subjects, largely supporting increasingly complex scientific
    developments. While the latter is important, it should not be at the expense
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other
    industrial countries in Europe and Asia, continue to encourage and advance
    the teaching of engineering. Both China and India, respectively, graduate
    six and eight times as many traditional engineers as does the United States.
    Other industrial countries at minimum maintain their output, while America
    suffers an increasingly serious decline in the number of engineering graduates
    and a lack of well-educated engineers.\\n Summary:America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .
    \\n
        {prompt}.\\n Summary:\\n
        """,
        return_full_text=False,
        max_length=350,
    )
    return few_shot
summarize_with_few_shot(movie_introduction)

"""도전과제2. 감성 분석을 할 수 있는 distilbert-base-uncased-finetuned-sst-2-english 인코더 모델을 사용해 어떤 결과를 얻을 수 있는지 확인해보자."""

from transformers import AutoModelForSequenceClassification, AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

input_ids = tokenizer(
    "This movie was so awesome!", truncation=True, return_tensors="pt"
).input_ids
classifier_output = model(input_ids)
classifier_output

predicted_class_id = classifier_output.logits.argmax().item()
predicted_class_id

model.config.id2label

model.config.id2label[predicted_class_id]

def classify_motions_from_review(reviews):
    input_ids = tokenizer(
        reviews["text"], truncation=True, return_tensors="pt"
    ).input_ids
    classifier_output = model(input_ids)
    reviews["motion"] = model.config.id2label[classifier_output.logits.argmax().item()]
    return reviews

classify_motions_from_review({"text":"This movie was so terrible.."})

"""도전과제3. FAQ 시스템 만들기. sentence_transformers 라이브러리를 사용해 전체 입력 텍스트에 대한 임베딩을 출력하여 텍스트 간 의미 유사성을 판단하자."""

from sentence_transformers import SentenceTransformer, util

sentences = ["I'm happy", "I'm full of happiness"]
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

embedding_1 = model.encode(sentences[0], convert_to_tensor=True)
embedding_2 = model.encode(sentences[1], convert_to_tensor=True)

util.pytorch_cos_sim(embedding_1, embedding_2)

"""특정 주제에 관한 5개의 질문과 답변 목록을 작성해, 새로운 질문에 대해 가장 적절한 답변을 제공하는 FAQ 시스템을 구축해보자!"""

five_FAQ = {
    "What is the color of lemon?": "The color of fruit lemon is yellow.",
    "How is the taste of lemon?": "It taste so sour!",
    "Where is the best place for picnic in Korea?": "The large river called 'Han-Gang' is the most popluar place for picnic. I recommend there!",
    "Can you tell me who is the most popular k-pop group?": "The K-pop girl group called 'Aespa' is the most famous.",
    "What is the color of melon?": "The color of fruit melon is light-green, maybe.",
}

embedding_five_FAQ = model.encode(list(five_FAQ.values()), convert_to_tensor=True)
print(embedding_five_FAQ.shape)

user_Q = "What is the color of chocolate?"
embedding_Q = model.encode(user_Q, convert_to_tensor=True)
embedding_Q.shape

import numpy as np
similarity = -util.pytorch_cos_sim(embedding_Q, embedding_five_FAQ)[0]
top_3 = similarity.cpu().argsort()[:3]
for i, top_n in enumerate(top_3):
    print(
        f"Top {i+1} question (p={-similarity[top_n]}): {list(five_FAQ.keys())[top_n]}"
    )
    print(f"Answer: {list(five_FAQ.values())[top_n]}")

similarity = util.semantic_search(
    embedding_Q, embedding_five_FAQ, top_k=3
)[0]
for i, result in enumerate(similarity):
    corpus_id = result["corpus_id"]
    score = result["score"]
    print(f"Top {i+1} question (p={score}): {list(five_FAQ.keys())[corpus_id]}")
    print(f"Answer: {list(five_FAQ.values())[corpus_id]}")