# -*- coding: utf-8 -*-
"""2.6 언어모델을 이용한 텍스트 생성 프로젝트.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbUEnUASjogCJf7M1uCr82mzu2-otUYZ
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
input_ids = tokenizer("Now, Let us eat delicious lunch! The menu is", return_tensors="pt").input_ids

import torch.nn.functional as F
import torch

def generate(
        model, tokenizer, input_ids, max_length=50, do_sample=False, top_k=0
):
    current_length = input_ids.size(1)

    for _ in range(max_length - current_length):
        outputs = model(input_ids)
        next_token_logits = outputs.logits[:,-1,:]

        if do_sample:
            if top_k>0:
                top_k_logits, _ = torch.topk(next_token_logits, top_k)
                min_top_k_value = top_k_logits[:, -1].unsqueeze(-1)
                next_token_logits = torch.where(
                    next_token_logits < min_top_k_value,
                    torch.tensor(float("-inf")),
                    next_token_logits,
                )
            probs = F.softmax(next_token_logits, dim=-1)
            next_token_id = torch.multinomial(probs, num_samples=1)
        else:
            next_token_id = torch.argmax(
                next_token_logits, dim=-1, keepdim=True
            )

        input_ids = torch.cat([input_ids, next_token_id], dim=-1)

        if next_token_id == tokenizer.eos_token_id:
            break
    return tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True)

generate(model, tokenizer, input_ids, do_sample=False)

generate(model, tokenizer, input_ids, do_sample=True)

generate(model, tokenizer, input_ids, do_sample=True, top_k=5)